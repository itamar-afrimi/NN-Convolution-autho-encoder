{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQG2DyRppMFF",
        "outputId": "9ce2f73b-26ca-41ae-d743-f865bc2b240f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.19.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "S2_8prFEw2bt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import wandb\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnJCBoXpw2bu"
      },
      "source": [
        "The output of torchvision datasets are PILImage images of range [0, 1].\n",
        "We transform them to Tensors of normalized range [-1, 1].\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gR8BQiCw2bv",
        "outputId": "9d5c2383-9236-42b8-f3a6-aa84246db1d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVtzZyO7w2bx"
      },
      "source": [
        "### 2. Define a Convolutional Neural Network\n",
        "Copy the neural network from the Neural Networks section before and modify it to\n",
        "take 3-channel images (instead of 1-channel images as it was defined).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "mmE5aKOdw2by"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, num_filters):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, num_filters, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(num_filters, num_filters*2, 5)\n",
        "        self.fc1 = nn.Linear(num_filters*2 * 5 * 5, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        return x\n",
        "class NetPool(nn.Module):\n",
        "    def __init__(self, num_filters=96):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, num_filters, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(num_filters, num_filters*2, 5)\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc1 = nn.Linear(num_filters*2, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        num_filters = self.conv1.out_channels\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = self.global_pool(x)\n",
        "        x = x.view(-1, num_filters*2)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class NetRelu(nn.Module):\n",
        "    def __init__(self, num_filters=96):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, num_filters, 5)\n",
        "        self.fc1 = nn.Linear(num_filters * 28 * 28, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class NetNon(nn.Module):\n",
        "    def __init__(self, num_filters=110):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, num_filters, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(num_filters, num_filters*2, 5)\n",
        "        self.fc1 = nn.Linear(num_filters*2 * 5 * 5, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.conv1(x))\n",
        "        x = self.pool(self.conv2(x))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class NetNonBig(nn.Module):\n",
        "    def __init__(self, num_filters=96):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, num_filters, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(num_filters, num_filters)\n",
        "        self.conv3 = nn.Conv2d(num_filters, num_filters)\n",
        "        self.conv4 = nn.Conv2d(num_filters, num_filters)\n",
        "        self.fc1 = nn.Linear(num_filters*2 * 5 * 5, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.conv1(x))\n",
        "        x = self.pool(self.conv2(x))\n",
        "        x = self.pool(self.conv3(x))\n",
        "        print(f'NetNonBig shape is {x.shape}')\n",
        "        x = self.pool(self.conv4(x))\n",
        "        print(f'NetNonBig shape is {x.shape}')\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW1td39mw2by"
      },
      "source": [
        "### 3. Define a Loss function and optimizer\n",
        "Let's use a Classification Cross-Entropy loss and SGD with momentum.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "PVPs-1cPw2bz"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "learning_rate = 0.001\n",
        "num_filters_list = [110]\n",
        "# 6, 12, 24, 48, 96"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7TZ7Krgw2bz"
      },
      "source": [
        "### 4. Train the network\n",
        "\n",
        "This is when things start to get interesting.\n",
        "We simply have to loop over our data iterator, and feed the inputs to the\n",
        "network and optimize.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "myNZlGfbw2bz"
      },
      "outputs": [],
      "source": [
        "def train(trainloader, testloader, num_epochs=5):\n",
        "  results = {}\n",
        "  \n",
        "  for num_filters in num_filters_list:\n",
        "      # Define the model, optimizer, and loss function\n",
        "      # model = NetNon(num_filters)\n",
        "\n",
        "      model = NetPool()\n",
        "      model = model.to(device)\n",
        "\n",
        "      optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "      wandb.watch(model, criterion, log=\"all\", log_freq=5)\n",
        "\n",
        "      # Train the model\n",
        "      num_iter = 0\n",
        "      for epoch in range(num_epochs):\n",
        "          wandb.log({\"epoch\": epoch}, step=num_iter)\n",
        "          running_loss = 0.0\n",
        "          for i, data in enumerate(trainloader, 0):\n",
        "              num_iter += 1\n",
        "              inputs, labels = data[0].to(device), data[1].to(device)\n",
        "              optimizer.zero_grad()\n",
        "              outputs = model(inputs)\n",
        "              loss = criterion(outputs, labels)\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              running_loss += loss.item()\n",
        "              if i % 200 == 199:    # print every 20 mini-batches\n",
        "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
        "                wandb.log({\"running loss {}\".format(num_filters): running_loss / 200, }, step=num_iter)\n",
        "                # wandb.log({'Num Filters': num_filters, 'Train Loss': running_loss/20})\n",
        "                running_loss = 0.0\n",
        "\n",
        "          epoch_loss = running_loss / len(trainloader)\n",
        "          wandb.log({f'Num Filters: {num_filters}, epoch Loss': epoch_loss})\n",
        "  test_model(model, criterion, num_filters)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, criterion, num_filters):\n",
        "\n",
        "  # Evaluate the model on the test set\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  test_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "      for i, data in enumerate(testloader, 0):\n",
        "          inputs, labels = data[0].to(device), data[1].to(device)\n",
        "          outputs = model(inputs)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "          loss = criterion(outputs, labels)\n",
        "          test_loss += loss.item()\n",
        "          if i % 20 == 19:    # print every 2000 mini-batches\n",
        "              print(f'test loss :  {test_loss / 20}' )\n",
        "              wandb.log({\n",
        "            'Num Filters': num_filters,\n",
        "            'Test Loss':test_loss /20})\n",
        "              test_loss = 0.0\n",
        "\n",
        "          \n",
        "  test_accuracy = 100 * correct / total\n",
        "  num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f'accuracy is : {test_accuracy}')\n",
        "  wandb.log({f'Num Filters: {num_filters}, Test Accuracy': test_accuracy, f'Num Filters: {num_filters}, Num Params': num_params})\n"
      ],
      "metadata": {
        "id": "sEOrShQRPdE7"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "kF-4WiXvzeEy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "03dd5400-a421-4e6a-b012-5ae5b3004487"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:addsol14) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">expert-brook-53</strong> at: <a href='https://wandb.ai/team-itamar/ex_1nn/runs/addsol14' target=\"_blank\">https://wandb.ai/team-itamar/ex_1nn/runs/addsol14</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230404_182900-addsol14/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:addsol14). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230404_183013-lkounkw0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/team-itamar/ex_1nn/runs/lkounkw0' target=\"_blank\">rare-meadow-54</a></strong> to <a href='https://wandb.ai/team-itamar/ex_1nn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/team-itamar/ex_1nn' target=\"_blank\">https://wandb.ai/team-itamar/ex_1nn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/team-itamar/ex_1nn/runs/lkounkw0' target=\"_blank\">https://wandb.ai/team-itamar/ex_1nn/runs/lkounkw0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   200] loss: 2.285\n",
            "[1,   400] loss: 2.258\n",
            "[1,   600] loss: 2.223\n",
            "[1,   800] loss: 2.229\n",
            "[1,  1000] loss: 2.171\n",
            "[1,  1200] loss: 2.153\n",
            "[1,  1400] loss: 2.128\n",
            "[1,  1600] loss: 2.109\n",
            "[1,  1800] loss: 2.132\n",
            "[1,  2000] loss: 2.097\n",
            "[1,  2200] loss: 2.106\n",
            "[1,  2400] loss: 2.044\n",
            "[1,  2600] loss: 2.078\n",
            "[1,  2800] loss: 2.043\n",
            "[1,  3000] loss: 2.017\n",
            "[1,  3200] loss: 1.966\n",
            "[1,  3400] loss: 2.015\n",
            "[1,  3600] loss: 2.001\n",
            "[1,  3800] loss: 1.930\n",
            "[1,  4000] loss: 1.910\n",
            "[1,  4200] loss: 1.891\n",
            "[1,  4400] loss: 1.849\n",
            "[1,  4600] loss: 1.893\n",
            "[1,  4800] loss: 1.855\n",
            "[1,  5000] loss: 1.850\n",
            "[1,  5200] loss: 1.787\n",
            "[1,  5400] loss: 1.790\n",
            "[1,  5600] loss: 1.761\n",
            "[1,  5800] loss: 1.768\n",
            "[1,  6000] loss: 1.711\n",
            "[1,  6200] loss: 1.668\n",
            "[1,  6400] loss: 1.731\n",
            "[1,  6600] loss: 1.697\n",
            "[1,  6800] loss: 1.703\n",
            "[1,  7000] loss: 1.663\n",
            "[1,  7200] loss: 1.665\n",
            "[1,  7400] loss: 1.691\n",
            "[1,  7600] loss: 1.583\n",
            "[1,  7800] loss: 1.646\n",
            "[1,  8000] loss: 1.611\n",
            "[1,  8200] loss: 1.545\n",
            "[1,  8400] loss: 1.649\n",
            "[1,  8600] loss: 1.547\n",
            "[1,  8800] loss: 1.592\n",
            "[1,  9000] loss: 1.596\n",
            "[1,  9200] loss: 1.583\n",
            "[1,  9400] loss: 1.597\n",
            "[1,  9600] loss: 1.553\n",
            "[1,  9800] loss: 1.582\n",
            "[1, 10000] loss: 1.562\n",
            "[1, 10200] loss: 1.579\n",
            "[1, 10400] loss: 1.502\n",
            "[1, 10600] loss: 1.550\n",
            "[1, 10800] loss: 1.490\n",
            "[1, 11000] loss: 1.533\n",
            "[1, 11200] loss: 1.531\n",
            "[1, 11400] loss: 1.504\n",
            "[1, 11600] loss: 1.513\n",
            "[1, 11800] loss: 1.575\n",
            "[1, 12000] loss: 1.463\n",
            "[1, 12200] loss: 1.468\n",
            "[1, 12400] loss: 1.462\n",
            "[2,   200] loss: 1.440\n",
            "[2,   400] loss: 1.422\n",
            "[2,   600] loss: 1.475\n",
            "[2,   800] loss: 1.478\n",
            "[2,  1000] loss: 1.544\n",
            "[2,  1200] loss: 1.488\n",
            "[2,  1400] loss: 1.463\n",
            "[2,  1600] loss: 1.426\n",
            "[2,  1800] loss: 1.429\n",
            "[2,  2000] loss: 1.439\n",
            "[2,  2200] loss: 1.447\n",
            "[2,  2400] loss: 1.443\n",
            "[2,  2600] loss: 1.451\n",
            "[2,  2800] loss: 1.431\n",
            "[2,  3000] loss: 1.443\n",
            "[2,  3200] loss: 1.413\n",
            "[2,  3400] loss: 1.380\n",
            "[2,  3600] loss: 1.427\n",
            "[2,  3800] loss: 1.347\n",
            "[2,  4000] loss: 1.385\n",
            "[2,  4200] loss: 1.398\n",
            "[2,  4400] loss: 1.401\n",
            "[2,  4600] loss: 1.380\n",
            "[2,  4800] loss: 1.402\n",
            "[2,  5000] loss: 1.409\n",
            "[2,  5200] loss: 1.373\n",
            "[2,  5400] loss: 1.394\n",
            "[2,  5600] loss: 1.314\n",
            "[2,  5800] loss: 1.411\n",
            "[2,  6000] loss: 1.358\n",
            "[2,  6200] loss: 1.306\n",
            "[2,  6400] loss: 1.418\n",
            "[2,  6600] loss: 1.351\n",
            "[2,  6800] loss: 1.370\n",
            "[2,  7000] loss: 1.347\n",
            "[2,  7200] loss: 1.361\n",
            "[2,  7400] loss: 1.320\n",
            "[2,  7600] loss: 1.399\n",
            "[2,  7800] loss: 1.337\n",
            "[2,  8000] loss: 1.314\n",
            "[2,  8200] loss: 1.345\n",
            "[2,  8400] loss: 1.325\n",
            "[2,  8600] loss: 1.354\n",
            "[2,  8800] loss: 1.343\n",
            "[2,  9000] loss: 1.306\n",
            "[2,  9200] loss: 1.275\n",
            "[2,  9400] loss: 1.297\n",
            "[2,  9600] loss: 1.270\n",
            "[2,  9800] loss: 1.290\n",
            "[2, 10000] loss: 1.318\n",
            "[2, 10200] loss: 1.418\n",
            "[2, 10400] loss: 1.297\n",
            "[2, 10600] loss: 1.331\n",
            "[2, 10800] loss: 1.250\n",
            "[2, 11000] loss: 1.303\n",
            "[2, 11200] loss: 1.334\n",
            "[2, 11400] loss: 1.328\n",
            "[2, 11600] loss: 1.311\n",
            "[2, 11800] loss: 1.310\n",
            "[2, 12000] loss: 1.280\n",
            "[2, 12200] loss: 1.292\n",
            "[2, 12400] loss: 1.287\n",
            "[3,   200] loss: 1.246\n",
            "[3,   400] loss: 1.256\n",
            "[3,   600] loss: 1.229\n",
            "[3,   800] loss: 1.281\n",
            "[3,  1000] loss: 1.197\n",
            "[3,  1200] loss: 1.300\n",
            "[3,  1400] loss: 1.261\n",
            "[3,  1600] loss: 1.212\n",
            "[3,  1800] loss: 1.217\n",
            "[3,  2000] loss: 1.228\n",
            "[3,  2200] loss: 1.198\n",
            "[3,  2400] loss: 1.246\n",
            "[3,  2600] loss: 1.248\n",
            "[3,  2800] loss: 1.223\n",
            "[3,  3000] loss: 1.211\n",
            "[3,  3200] loss: 1.216\n",
            "[3,  3400] loss: 1.223\n",
            "[3,  3600] loss: 1.174\n",
            "[3,  3800] loss: 1.272\n",
            "[3,  4000] loss: 1.208\n",
            "[3,  4200] loss: 1.201\n",
            "[3,  4400] loss: 1.183\n",
            "[3,  4600] loss: 1.140\n",
            "[3,  4800] loss: 1.267\n",
            "[3,  5000] loss: 1.240\n",
            "[3,  5200] loss: 1.201\n",
            "[3,  5400] loss: 1.204\n",
            "[3,  5600] loss: 1.210\n",
            "[3,  5800] loss: 1.262\n",
            "[3,  6000] loss: 1.139\n",
            "[3,  6200] loss: 1.176\n",
            "[3,  6400] loss: 1.218\n",
            "[3,  6600] loss: 1.236\n",
            "[3,  6800] loss: 1.167\n",
            "[3,  7000] loss: 1.241\n",
            "[3,  7200] loss: 1.239\n",
            "[3,  7400] loss: 1.218\n",
            "[3,  7600] loss: 1.255\n",
            "[3,  7800] loss: 1.250\n",
            "[3,  8000] loss: 1.190\n",
            "[3,  8200] loss: 1.207\n",
            "[3,  8400] loss: 1.230\n",
            "[3,  8600] loss: 1.272\n",
            "[3,  8800] loss: 1.179\n",
            "[3,  9000] loss: 1.203\n",
            "[3,  9200] loss: 1.207\n",
            "[3,  9400] loss: 1.155\n",
            "[3,  9600] loss: 1.267\n",
            "[3,  9800] loss: 1.112\n",
            "[3, 10000] loss: 1.218\n",
            "[3, 10200] loss: 1.161\n",
            "[3, 10400] loss: 1.207\n",
            "[3, 10600] loss: 1.168\n",
            "[3, 10800] loss: 1.190\n",
            "[3, 11000] loss: 1.166\n",
            "[3, 11200] loss: 1.196\n",
            "[3, 11400] loss: 1.220\n",
            "[3, 11600] loss: 1.121\n",
            "[3, 11800] loss: 1.129\n",
            "[3, 12000] loss: 1.145\n",
            "[3, 12200] loss: 1.121\n",
            "[3, 12400] loss: 1.171\n",
            "[4,   200] loss: 1.122\n",
            "[4,   400] loss: 1.090\n",
            "[4,   600] loss: 1.210\n",
            "[4,   800] loss: 1.168\n",
            "[4,  1000] loss: 1.054\n",
            "[4,  1200] loss: 1.105\n",
            "[4,  1400] loss: 1.033\n",
            "[4,  1600] loss: 1.168\n",
            "[4,  1800] loss: 1.108\n",
            "[4,  2000] loss: 1.181\n",
            "[4,  2200] loss: 1.096\n",
            "[4,  2400] loss: 1.155\n",
            "[4,  2600] loss: 1.146\n",
            "[4,  2800] loss: 1.144\n",
            "[4,  3000] loss: 1.034\n",
            "[4,  3200] loss: 1.135\n",
            "[4,  3400] loss: 1.095\n",
            "[4,  3600] loss: 1.136\n",
            "[4,  3800] loss: 1.094\n",
            "[4,  4000] loss: 1.177\n",
            "[4,  4200] loss: 1.110\n",
            "[4,  4400] loss: 1.072\n",
            "[4,  4600] loss: 1.044\n",
            "[4,  4800] loss: 1.081\n",
            "[4,  5000] loss: 1.095\n",
            "[4,  5200] loss: 1.121\n",
            "[4,  5400] loss: 1.074\n",
            "[4,  5600] loss: 1.160\n",
            "[4,  5800] loss: 1.082\n",
            "[4,  6000] loss: 1.037\n",
            "[4,  6200] loss: 1.090\n",
            "[4,  6400] loss: 1.118\n",
            "[4,  6600] loss: 1.105\n",
            "[4,  6800] loss: 1.031\n",
            "[4,  7000] loss: 1.144\n",
            "[4,  7200] loss: 1.144\n",
            "[4,  7400] loss: 1.120\n",
            "[4,  7600] loss: 1.132\n",
            "[4,  7800] loss: 1.052\n",
            "[4,  8000] loss: 1.120\n",
            "[4,  8200] loss: 1.069\n",
            "[4,  8400] loss: 1.097\n",
            "[4,  8600] loss: 1.086\n",
            "[4,  8800] loss: 1.068\n",
            "[4,  9000] loss: 1.108\n",
            "[4,  9200] loss: 1.035\n",
            "[4,  9400] loss: 1.057\n",
            "[4,  9600] loss: 1.071\n",
            "[4,  9800] loss: 1.091\n",
            "[4, 10000] loss: 1.034\n",
            "[4, 10200] loss: 1.102\n",
            "[4, 10400] loss: 1.035\n",
            "[4, 10600] loss: 1.096\n",
            "[4, 10800] loss: 0.993\n",
            "[4, 11000] loss: 1.132\n",
            "[4, 11200] loss: 1.080\n",
            "[4, 11400] loss: 1.041\n",
            "[4, 11600] loss: 1.057\n",
            "[4, 11800] loss: 1.130\n",
            "[4, 12000] loss: 1.133\n",
            "[4, 12200] loss: 1.043\n",
            "[4, 12400] loss: 1.085\n",
            "[5,   200] loss: 1.054\n",
            "[5,   400] loss: 1.041\n",
            "[5,   600] loss: 0.980\n",
            "[5,   800] loss: 1.041\n",
            "[5,  1000] loss: 1.088\n",
            "[5,  1200] loss: 0.957\n",
            "[5,  1400] loss: 1.053\n",
            "[5,  1600] loss: 1.026\n",
            "[5,  1800] loss: 1.032\n",
            "[5,  2000] loss: 1.048\n",
            "[5,  2200] loss: 1.053\n",
            "[5,  2400] loss: 1.012\n",
            "[5,  2600] loss: 1.103\n",
            "[5,  2800] loss: 1.031\n",
            "[5,  3000] loss: 0.998\n",
            "[5,  3200] loss: 1.038\n",
            "[5,  3400] loss: 0.977\n",
            "[5,  3600] loss: 1.002\n",
            "[5,  3800] loss: 1.028\n",
            "[5,  4000] loss: 1.007\n",
            "[5,  4200] loss: 1.074\n",
            "[5,  4400] loss: 1.027\n",
            "[5,  4600] loss: 0.972\n",
            "[5,  4800] loss: 0.965\n",
            "[5,  5000] loss: 0.993\n",
            "[5,  5200] loss: 1.105\n",
            "[5,  5400] loss: 1.044\n",
            "[5,  5600] loss: 1.012\n",
            "[5,  5800] loss: 0.907\n",
            "[5,  6000] loss: 1.030\n",
            "[5,  6200] loss: 0.984\n",
            "[5,  6400] loss: 0.996\n",
            "[5,  6600] loss: 1.008\n",
            "[5,  6800] loss: 1.014\n",
            "[5,  7000] loss: 0.960\n",
            "[5,  7200] loss: 1.025\n",
            "[5,  7400] loss: 1.024\n",
            "[5,  7600] loss: 1.023\n",
            "[5,  7800] loss: 1.008\n",
            "[5,  8000] loss: 1.001\n",
            "[5,  8200] loss: 0.996\n",
            "[5,  8400] loss: 1.024\n",
            "[5,  8600] loss: 1.030\n",
            "[5,  8800] loss: 0.986\n",
            "[5,  9000] loss: 1.018\n",
            "[5,  9200] loss: 1.023\n",
            "[5,  9400] loss: 0.994\n",
            "[5,  9600] loss: 1.040\n",
            "[5,  9800] loss: 1.017\n",
            "[5, 10000] loss: 0.944\n",
            "[5, 10200] loss: 1.052\n",
            "[5, 10400] loss: 0.969\n",
            "[5, 10600] loss: 0.927\n",
            "[5, 10800] loss: 0.959\n",
            "[5, 11000] loss: 1.015\n",
            "[5, 11200] loss: 1.047\n",
            "[5, 11400] loss: 1.008\n",
            "[5, 11600] loss: 0.925\n",
            "[5, 11800] loss: 1.043\n",
            "[5, 12000] loss: 0.983\n",
            "[5, 12200] loss: 1.005\n",
            "[5, 12400] loss: 0.970\n",
            "test loss :  0.9989930719137192\n",
            "test loss :  0.9124762065708637\n",
            "test loss :  1.0983461648225785\n",
            "test loss :  1.0073491871356963\n",
            "test loss :  1.0488244220614433\n",
            "test loss :  1.0834271505475044\n",
            "test loss :  0.7220312025398016\n",
            "test loss :  0.7580767497420311\n",
            "test loss :  1.040624088048935\n",
            "test loss :  1.0872466422617435\n",
            "test loss :  0.907511143386364\n",
            "test loss :  0.9390490435063839\n",
            "test loss :  0.7396410025656224\n",
            "test loss :  0.9486162308603525\n",
            "test loss :  0.9211892552673817\n",
            "test loss :  1.0721059679985045\n",
            "test loss :  0.8226468428969383\n",
            "test loss :  0.9439645759761334\n",
            "test loss :  1.0483312033116818\n",
            "test loss :  1.200822839140892\n",
            "test loss :  0.8143051415681839\n",
            "test loss :  1.0743864819407463\n",
            "test loss :  0.9528735291212798\n",
            "test loss :  0.9275741264224052\n",
            "test loss :  1.1067633897066116\n",
            "test loss :  0.9949671559035778\n",
            "test loss :  0.902453963458538\n",
            "test loss :  0.8927357427775859\n",
            "test loss :  1.1248163715004922\n",
            "test loss :  1.076923445612192\n",
            "test loss :  0.908004792034626\n",
            "test loss :  1.2973171144723892\n",
            "test loss :  0.9946833200752735\n",
            "test loss :  0.9950435720384121\n",
            "test loss :  0.9632869347929954\n",
            "test loss :  0.8730531454086303\n",
            "test loss :  0.9217086300253868\n",
            "test loss :  0.8989831879734993\n",
            "test loss :  1.0095184683799743\n",
            "test loss :  0.9872677206993103\n",
            "test loss :  1.0216014936566353\n",
            "test loss :  1.1081154108047486\n",
            "test loss :  0.9096372880041599\n",
            "test loss :  1.0326678313314914\n",
            "test loss :  0.8061032995581627\n",
            "test loss :  1.1401535436511039\n",
            "test loss :  1.119981336593628\n",
            "test loss :  0.927236194908619\n",
            "test loss :  0.8392229214310646\n",
            "test loss :  0.8461841166019439\n",
            "test loss :  0.9831187069416046\n",
            "test loss :  1.1290548287332058\n",
            "test loss :  0.8226711079478264\n",
            "test loss :  0.9317842580378055\n",
            "test loss :  0.7967020407319069\n",
            "test loss :  0.8916638456285\n",
            "test loss :  0.8662735864520072\n",
            "test loss :  1.0079751089215279\n",
            "test loss :  0.8012376129627228\n",
            "test loss :  0.9310697942972184\n",
            "test loss :  0.8621437296271324\n",
            "test loss :  0.8287413939833641\n",
            "test loss :  0.944670532643795\n",
            "test loss :  1.0562392637133597\n",
            "test loss :  0.9672737285494805\n",
            "test loss :  0.9223273456096649\n",
            "test loss :  1.0267039462924004\n",
            "test loss :  1.1032752264291048\n",
            "test loss :  0.9140073746442795\n",
            "test loss :  1.1480336844921113\n",
            "test loss :  1.0743266422301532\n",
            "test loss :  0.6863135788589716\n",
            "test loss :  1.034407913684845\n",
            "test loss :  1.194036877155304\n",
            "test loss :  0.9235758418217301\n",
            "test loss :  1.1618768200278282\n",
            "test loss :  1.0672916341573\n",
            "test loss :  0.9716948576271534\n",
            "test loss :  0.8553681045770645\n",
            "test loss :  0.8923994496464729\n",
            "test loss :  0.9694735996425152\n",
            "test loss :  1.0597977593541146\n",
            "test loss :  0.955975030362606\n",
            "test loss :  1.0092327877879144\n",
            "test loss :  1.1439893633127212\n",
            "test loss :  1.0929275408387185\n",
            "test loss :  1.0602664947509766\n",
            "test loss :  1.1945350259542464\n",
            "test loss :  0.8127414181828498\n",
            "test loss :  0.9020704325288534\n",
            "test loss :  0.8610598728060722\n",
            "test loss :  0.6826431468129158\n",
            "test loss :  0.9367720425128937\n",
            "test loss :  1.045100063085556\n",
            "test loss :  0.8612334582954645\n",
            "test loss :  1.0207846254110335\n",
            "test loss :  0.9524312511086463\n",
            "test loss :  1.1356956973671912\n",
            "test loss :  1.020014514774084\n",
            "test loss :  0.8920975469052792\n",
            "test loss :  1.0359865993261337\n",
            "test loss :  0.9143811400979758\n",
            "test loss :  0.9922522336244584\n",
            "test loss :  1.0166549861431122\n",
            "test loss :  1.0399709790945053\n",
            "test loss :  0.9201341077685357\n",
            "test loss :  1.0030634440481663\n",
            "test loss :  0.9841760858893395\n",
            "test loss :  0.9953274488449096\n",
            "test loss :  0.9744275353848935\n",
            "test loss :  1.1893438167870045\n",
            "test loss :  1.0066718250513076\n",
            "test loss :  0.924677912145853\n",
            "test loss :  0.8222626574337483\n",
            "test loss :  0.7847070425748826\n",
            "test loss :  0.9882418833673\n",
            "test loss :  0.9249373808503151\n",
            "test loss :  1.0368119478225708\n",
            "test loss :  1.0001989513635636\n",
            "test loss :  0.8607890754938126\n",
            "test loss :  0.9309337973594666\n",
            "test loss :  1.073826852440834\n",
            "test loss :  1.0569520488381385\n",
            "test loss :  1.1065456345677376\n",
            "test loss :  1.1004221141338348\n",
            "accuracy is : 66.12\n"
          ]
        }
      ],
      "source": [
        "wandb.init(project='ex_1nn')\n",
        "train(trainloader, testloader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}